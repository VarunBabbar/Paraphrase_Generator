{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import json \n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# # Loading all training examples, validation examples, and vocabulary\n",
    "\n",
    "# with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Paraphrase_Generator/quora_raw_train.json') as f:\n",
    "#     training_examples = json.load(f)\n",
    "    \n",
    "# with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Paraphrase_Generator/quora_raw_val.json') as f:\n",
    "#     val_examples = json.load(f)\n",
    "\n",
    "# with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Paraphrase_Generator/quora_data_prepro.json') as f:\n",
    "#     vocab = json.load(f)['ix_to_word'] # Vocab = {1: 'the', 2: 'sauce' .....}\n",
    "# vocab_inv = {word:number for number,word in list(vocab.items())} # Inverted dictionary created   Vocab_inv = {'the':1, 'sauce':2, .....}\n",
    "\n",
    "# X_train_prelim = [training_examples[i]['question1'] for i in range(len(training_examples))] # Training examples before preprocessing\n",
    "# Y_train_prelim = [training_examples[i]['question'] for i in range(len(training_examples))]\n",
    "\n",
    "# X_val_prelim = [val_examples[i]['question1'] for i in range(len(val_examples))]\n",
    "# Y_val_prelim = [val_examples[i]['question'] for i in range(len(val_examples))]\n",
    "\n",
    "# def load_glove_embeddings(filename=\"glove.6B.100d.txt\"):\n",
    "#     try:\n",
    "#         with open(\"/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Paraphrase_Generator/embeddings\", \"rb\") as f:\n",
    "#             embeddings = pickle.load(f)\n",
    "#         return embeddings\n",
    "#     except:\n",
    "#         lines = open(filename).readlines()\n",
    "#         print(len(lines))\n",
    "#         embeddings = {}\n",
    "#         a = 0\n",
    "#         for line in lines:\n",
    "#             a +=1\n",
    "#             print(a)\n",
    "#             word = line.split()[0]\n",
    "#             embedding = list(map(float, line.split()[1:]))\n",
    "#             if word in vocab.values():\n",
    "#                 embeddings[int(vocab_inv[word])] = embedding # This is creating a dictionary with indexes corresponding to the position of the particular word in the vocabulary\n",
    "#                 # embeddings = {1:embedding of 'the', 2: embedding of 'sauce'....}\n",
    "#         return embeddings\n",
    "\n",
    "# def tokenize_embed(X,Y): # Doing this for training and validation_datasets\n",
    "#     X_train = []\n",
    "#     Y_train = []\n",
    "#     for i in range(len(X)): # This is basically tokenising each sentence, and converting it into a list of word embeddings. X corresponds to a training example and Y corresponds to its paraphrase\n",
    "#         print(i)\n",
    "#         sentence_x = X[i]\n",
    "#         sentence_y = Y[i]\n",
    "#         tokensx = word_tokenize(sentence_x.lower())\n",
    "#         tokensy = word_tokenize(sentence_y.lower())\n",
    "#         vectorx = []\n",
    "#         vectory = []\n",
    "#         for word in tokensx:\n",
    "#             try:\n",
    "#                 vectorx.append(int(vocab_inv[word]))\n",
    "#             except:\n",
    "#                 vectorx.append(258) # If the word is not in the embeddings given, then add a ',' instead. (Need to discuss this)\n",
    "#         for word in tokensy:\n",
    "#             try:\n",
    "#                 vectory.append(int(vocab_inv[word]))\n",
    "#             except:\n",
    "#                 vectory.append(258)\n",
    "#         X_train.append(vectorx)\n",
    "#         Y_train.append(vectory)\n",
    "#     return X_train,Y_train\n",
    "\n",
    "# def clip_pad_input_sequences(X,Y,min_sentence_length,max_sentence_length): # Doing this for training and validation datasets\n",
    "#     X_training_clipped = []\n",
    "#     Y_training_clipped = []\n",
    "#     for i in range(len(X)):\n",
    "#         print(i,X[i])\n",
    "#         if len(X[i]) >= min_sentence_length and len(X[i]) <= max_sentence_length and len(Y[i]) >= min_sentence_length and len(Y[i]) <= max_sentence_length:\n",
    "#             X_training_clipped.append(torch.Tensor(X[i]))\n",
    "#             Y_training_clipped.append(torch.Tensor(Y[i]))\n",
    "#     padded_X_training_clipped = pad_sequence(X_training_clipped,padding_value = vocab_size)\n",
    "#     padded_Y_training_clipped = pad_sequence(X_training_clipped,padding_value = vocab_size)\n",
    "#     return padded_X_training_clipped,padded_Y_training_clipped\n",
    "\n",
    "# def find_closest_embedding(output,vocab,embedded_words): # Need this because we probably need to find the closest embedding to a given embedding and thereby find the word\n",
    "#     return\n",
    "\n",
    "# min_sentence_length = 7\n",
    "# max_sentence_length = 16\n",
    "# vocab_size = len(vocab)\n",
    "# # embedded_words = load_glove_embeddings()\n",
    "\n",
    "# X_train,Y_train = clip_pad_input_sequences(*tokenize_embed(X_train_prelim,Y_train_prelim,),min_sentence_length,max_sentence_length)\n",
    "# X_valid,Y_valid = clip_pad_input_sequences(*tokenize_embed(X_val_prelim,Y_val_prelim,),min_sentence_length,max_sentence_length)\n",
    "\n",
    "\n",
    "input_size = 100 # Size of embedding\n",
    "hidden_size = 256 # Can set this to be a custom value\n",
    "num_layers = 1 # Can change this\n",
    "bs = 128\n",
    "vocab_size = len(vocab)\n",
    "dropout = 0.2\n",
    "bidirectional = False\n",
    "max_sentence_length = 16 # Max number of words in a sentence\n",
    "\n",
    "class Encoder(nn.Module): # Encoder with a single layer LSTM \n",
    "    def __init__(self,max_sentence_length,input_size,hidden_size,dropout,vocab_size,num_layers,bs,bidirectional= False):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.bs = bs\n",
    "        self.num_layers = num_layers*2 if bidirectional else num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding(vocab_size,input_size,sparse=False) # Lookup table with word vectors and word index in vocab\n",
    "#         self.linear = nn.Sequential(nn.Linear(input_size,linear_output_size),nn.Relu(),nn.Linear(linear_output_size,input_size))\n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, num_layers, batch_first=False,bidirectional = self.bidirectional,dropout = self.dropout) # LSTM moodule\n",
    "        self.init_weights()\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        \n",
    "    def forward(self, x, prev_state,indices_given = True):  # X corresponds to the index of the word in the vocabulary (so X can be 2 or 49 etc, and embedding(x) will give the word vector of the 2nd or 49th word in the vocabulary)\n",
    "        if indices_given:\n",
    "            embed = self.embedding(x) # Looking up embeddings\n",
    "        else:\n",
    "            embed = x.float()\n",
    "        embed = torch.reshape(torch.Tensor(embed),(self.max_sentence_length,self.bs,self.input_size))\n",
    "        output, curr_state = self.lstm_layer(embed, prev_state)\n",
    "        return output,curr_state,embed\n",
    "    \n",
    "    def init_weights(self): # Initialising weights\n",
    "        init.orthogonal_(self.lstm_layer.weight_ih_l0)\n",
    "        init.uniform_(self.lstm_layer.weight_hh_l0, a=-0.01, b=0.01) # Values taken from the paper\n",
    "        embedding_weights = torch.FloatTensor(self.vocab_size+1, self.input_size)\n",
    "        init.uniform_(embedding_weights, a=-0.25, b=0.25) # Values taken from the paper\n",
    "        for k,v in embedded_words.items():\n",
    "            embedding_weights[k,:] = torch.FloatTensor(v)\n",
    "        embedding_weights[0] = torch.FloatTensor([0]*self.input_size)\n",
    "        embedding_weights[vocab_size] = torch.FloatTensor([0]*self.input_size) # Equivalent embedding for \n",
    "        del self.embedding.weight # Deleting any pre-initialised embeddings in nn.embedding.weights and replacing it with our known word embeddings\n",
    "        self.embedding.weight = nn.Parameter(embedding_weights)\n",
    "    \n",
    "    def train(self,x): # Batch feedforward across all training examples\n",
    "        curr_state = (torch.zeros(self.num_layers,self.bs,self.hidden_size),torch.zeros(self.num_layers,self.bs,self.hidden_size)) # Initialization of hidden and cell state\n",
    "        for i in range(max(x.shape)//bs):\n",
    "            if i < max(x.shape)//bs-1:\n",
    "                wordx = x[:,i:i+bs]\n",
    "                output, curr_state = encoder.forward(wordx.long(),curr_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, Encoder, embedding_dims, hidden_size, num_layers = 1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.Encoder = Encoder\n",
    "\n",
    "        ### QUESTION: What dimension for across q?\n",
    "        self.softmax = nn.Softmax(dim = -1)                 # or 2?\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.init_weights(embedding_dims, hidden_size, num_layers)\n",
    "\n",
    "    def local_loss_contrib(self, q_hat, q_real):\n",
    "        ### QUESTION: In the paper they have, what looks like, average across text, right?\n",
    "        sentence_length = q_hat.shape[0]\n",
    "\n",
    "        \n",
    "        s = 0\n",
    "        \n",
    "        for i in range(q_hat.shape[0]):\n",
    "            \n",
    "            dot = torch.dot(torch.log(q_hat[i]), q_real[i])\n",
    "            s += dot\n",
    "\n",
    "        return -(1/sentence_length)*s\n",
    "\n",
    "\n",
    "    def init_weights(self, embedding_dims, hidden_size, num_layers):\n",
    "        \n",
    "        # Size of batch = [None, sentence size, embedding dims]  :. size of Wd = [embedding dims, embedding dims] to map to same space\n",
    "        self.Wd = nn.Linear(in_features = embedding_dims, out_features = embedding_dims, bias = False)\n",
    "        self.Wv = nn.Linear(in_features = hidden_size, out_features = embedding_dims, bias = False)\n",
    "\n",
    "        # LSTM init with size [input_size, hidden_size, num_layers]\n",
    "        self.lstm = nn.LSTM(embedding_dims, hidden_size, num_layers)        \n",
    "    \n",
    "    def forward(self, sentence, q_real,indices_given=True):\n",
    "        \"\"\"\n",
    "        A forward pass on the LSTM decoder. Input and ground truth come in as matrix of indicies, later embedded to size [sentence length, batch size, embedding dims]\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialise hidden layer and cell state, size = [num_layers*num_directions, batch, hidden_size]\n",
    "\n",
    "        h0 = torch.zeros((self.num_layers, sentence.size()[1], self.hidden_size))\n",
    "        c0 = torch.zeros((self.num_layers, sentence.size()[1], self.hidden_size))\n",
    "\n",
    "        sentence = sentence.long()\n",
    "        \n",
    "        _, (h0, c0),sentence = self.Encoder.forward(sentence, (h0, c0),indices_given)\n",
    "        q_real = self.Encoder.embedding(q_real.long())\n",
    "\n",
    "        # dt = Wd * qt = d-weights * previously predicted word\n",
    "        dt = self.Wd(sentence)\n",
    "\n",
    "        # h[t+1] = LSTM(dt, ht)          LSTM output paramter = [sentence length, batch, hidden_size*num_directions]\n",
    "        out, (hn, cn) = self.lstm(dt, (h0, c0))\n",
    "        \n",
    "        # Out has dims: [sentence length, batch, embedding_dims]    \n",
    "        p = self.Wv(out)\n",
    "\n",
    "        # Softmax p to get q_hat\n",
    "        q_hat = self.softmax(p)\n",
    "        q_real_mean = torch.mean(q_real, dim = 1)\n",
    "        q_hat_mean = torch.mean(q_hat, dim = 1)\n",
    "\n",
    "        # CROSS ENTROPY TAKEN OVER dim 0 (i.e. sentence length)\n",
    "        # This is one of the terms at in the L_local summation, which must later be added up and divided\n",
    "        local_loss = self.local_loss_contrib(q_hat_mean, q_real_mean) /sentence.size()[1]\n",
    "        \n",
    "        return local_loss, q_hat,q_real,(h0,c0)\n",
    "\n",
    "    def train(self, num_epochs, num_iterations, learning_rate):\n",
    "        self.optimiser = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(num_iterations):\n",
    "                pass ### waiting for encoder output\n",
    "\n",
    "\n",
    "# def trial_forward_run():\n",
    "#     embedding_dims = 100\n",
    "#     hidden_size = 20\n",
    "\n",
    "#     decoder = Decoder(embedding_dims, hidden_size)\n",
    "\n",
    "#     inp = torch.randn(5, 3, embedding_dims)\n",
    "#     gtt = torch.randn(5, 3, embedding_dims)\n",
    "\n",
    "#     loss, q_hat = decoder.forward(inp, gtt)\n",
    "#     print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(-0.0561, grad_fn=<DivBackward0>)\n",
      "tensor(7.1333, grad_fn=<AddBackward0>)\n",
      "1\n",
      "tensor(-0.1659, grad_fn=<DivBackward0>)\n",
      "tensor(5011.2700, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-81826424b962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparamters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-136-81826424b962>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, optimizer)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from torchvision.transforms import Normalize\n",
    "input_size = 100\n",
    "embedding_dims = 100\n",
    "hidden_size_enc = 128\n",
    "hidden_size_dec = 128\n",
    "num_layers = 1\n",
    "vocab_size = len(vocab)\n",
    "dropout = 0.2\n",
    "max_sentence_length = 16\n",
    "\n",
    "class Discriminator():\n",
    "    def __init__(self,encoder):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.encoder = encoder # encoder module\n",
    "        \n",
    "    def forward(self,ground_truth_sentence,predicted_sentence,prev_state,indices_given = True): # Prev state is the hidden state and cell state of the encoder at the given timestep\n",
    "        _,(encoded_hidden_state_gt,encoded_cell_state_gt),_= encoder.forward(ground_truth_sentence.long(),prev_state,indices_given)\n",
    "        _,(encoded_hidden_state_pred,encoded_cell_state_pred),_ = encoder.forward(predicted_sentence.long(),prev_state,indices_given)\n",
    "        loss = self.global_loss(encoded_hidden_state_gt,encoded_hidden_state_pred)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def backward(self): # Have to call loss.backward()\n",
    "        return 0\n",
    "    \n",
    "    def global_loss(self,encoded_gt,encoded_pred): # Have to implement global loss_function\n",
    "        loss = 0\n",
    "        for i in range(bs):\n",
    "            for j in range(bs):\n",
    "                dotproduct = encoded_pred[:,i,:]*encoded_gt[:,j,:]-encoded_pred[:,i,:]*encoded_gt[:,i,:]\n",
    "#                 dotproduct = (dotproduct - torch.max(dotproduct))/(torch.max(dotproduct)-torch.min(dotproduct))\n",
    "                dotproduct = dotproduct.sum(dim=1)\n",
    "                loss += max(dotproduct[0],0)\n",
    "        return loss\n",
    "learning_rate = 0.001\n",
    "encoder = Encoder(max_sentence_length,input_size,hidden_size,dropout,vocab_size,num_layers,bs,bidirectional= False)\n",
    "decoder = Decoder(encoder,embedding_dims, hidden_size)\n",
    "discriminator = Discriminator(encoder)\n",
    "params = chain(decoder.parameters(),encoder.parameters())\n",
    "optimizer = torch.optim.RMSprop(decoder.parameters(), lr=learning_rate)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# model = decoder.to(device)\n",
    "# print(summary(model,(16,128,100)))\n",
    "def train(num_epochs,optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(X_train.shape[1]//bs):\n",
    "            print(i)\n",
    "            x = X_train[:,i:i+bs]\n",
    "            y = Y_train[:,i:i+bs]\n",
    "            local_loss, q_hat,q_real,prev_state = decoder.forward(x, y)\n",
    "            global_loss = discriminator.forward(q_real,q_hat,prev_state,indices_given=False) \n",
    "            total_loss = local_loss + global_loss\n",
    "            print(local_loss)\n",
    "            print(total_loss)\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(decoder.paramters())\n",
    "train(5,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
