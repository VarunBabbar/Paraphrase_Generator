{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW CLASS CALLED PGEN CREATED ###\n",
    "#### I HAVENT OPTIMIZED THIS BECAUSE IM NOT FEELING TOO WELL, BUT ILL GET ON IT TOMORROW ####\n",
    "#### SO ESSENTIALLY IN THIS CASE INSTEAD OF USING WORD2VEC WE ARE GENERATING OUR EMBEDDINGS DIRECTLY USING THE NN.EMBEDDING MODULE ######\n",
    "#### THE PAPER DID A SIMILAR THING #####\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "import fastai\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import json \n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class PGen(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dims, hidden_size, dropout,vocab_size,input_size,num_layers):\n",
    "        super(PGen, self).__init__()\n",
    "\n",
    "        # Hyper-parameters\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.hsize_decoder = int(self.hidden_size/2)\n",
    "        # Encoder\n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.embedding_dims,sparse=False) # Lookup table with word vectors and word index in vocab\n",
    "#         self.linear = nn.Sequential(nn.Linear(input_size,linear_output_size),nn.Relu(),nn.Linear(linear_output_size,input_size))\n",
    "        self.lstm_encoder = nn.GRU(self.embedding_dims, self.hidden_size, self.num_layers, batch_first=True, bidirectional = False,dropout = self.dropout) # LSTM module\n",
    "        self.encoder = nn.Sequential(self.embedding,self.lstm_encoder)\n",
    "        \n",
    "        # Decoder\n",
    "        self.linear_out = nn.Linear(in_features = self.hsize_decoder,out_features = self.input_size,bias = False)\n",
    "        self.lstm_decoder = nn.LSTM(self.hidden_size, self.hsize_decoder, self.num_layers, bidirectional = False, dropout = self.dropout,batch_first = True) \n",
    "        self.decoder_softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "        # Final Module\n",
    "        self.lstm_linear_decoder = nn.Sequential(self.lstm_decoder)\n",
    "        self.final_layer_decoder = nn.Sequential(self.linear_out,self.decoder_softmax)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def local_loss_contrib(self, q_hat, q_real): ### MAKE SURE THIS IS CORRECT\n",
    "        ### QUESTION: In the paper they have, what looks like, average across text, right?\n",
    "        sentence_length = q_hat.shape[0]\n",
    "        s = torch.log(q_hat)*q_real\n",
    "#         s = 0\n",
    "#         epsilon = 0.001\n",
    "#         for i in range(q_hat.shape[0]):\n",
    "#             dot = torch.dot(torch.log(q_hat[i,:]+epsilon), q_real[i,:])\n",
    "#             s += dot\n",
    "# #             print(torch.log(q_hat[i,:]+epsilon),q_hat[i,:])\n",
    "        return torch.abs((1/sentence_length)*s)\n",
    "\n",
    "    def init_weights(self): ##\n",
    "        # For the encoder\n",
    "        init.orthogonal_(self.lstm_encoder.weight_ih_l0)\n",
    "        init.uniform_(self.lstm_encoder.weight_hh_l0, a=-0.01, b=0.01) # Values taken from the paper\n",
    "        # For the decoder\n",
    "        init.orthogonal_(self.lstm_decoder.weight_ih_l0)\n",
    "        init.uniform_(self.lstm_decoder.weight_hh_l0, a=-0.01, b=0.01) \n",
    "        \n",
    "    \n",
    "    def forward(self, sentence, q_real,indices_given=True): #### GET THIS TO WORK ####\n",
    "        \"\"\"\n",
    "        A forward pass on the encoder and decoder module. Input and ground truth come in as matrix of indicies, later embedded to size [sentence length, batch size, embedding dims]\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialise hidden layer and cell state, size = [num_layers*num_directions, batch, hidden_size]\n",
    "        print(\"Hi\")\n",
    "        _,encoded_sentence = self.encoder(sentence) # PROBLEMS COMING HERE\n",
    "        print(encoded_sentence.shape)\n",
    "        encoded_sentence = torch.reshape(encoded_sentence,(bs,num_layers,hidden_size))\n",
    "        print(encoded_sentence.shape)\n",
    "        decoded_mid,_ = self.lstm_linear_decoder(encoded_sentence)\n",
    "        decoded_sentence = self.final_layer_decoder(decoded_mid)\n",
    "        \n",
    "        print(decoded_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size # Size of embedding\n",
    "hidden_size = 256 # Can set this to be a custom value\n",
    "num_layers = 1 # Can change this\n",
    "bs = 128\n",
    "# vocab_size = len(vocab)\n",
    "dropout = 0.0\n",
    "bidirectional = False\n",
    "max_sentence_length = 28 # Max number of words in a sentence\n",
    "embedding_dims = 100\n",
    "\n",
    "\n",
    "a = PGen(embedding_dims, hidden_size, dropout,vocab_size,input_size,num_layers)\n",
    "a.forward(X_train_final,Y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DONT RUN THIS RIGHT NOW #### \n",
    "##### THIS IS THE OLD VERSION OF OUR PREPROCESSING STEPS ######\n",
    "##### CHECK OUT THE CODE AT THE BOTTOM FIRST ######\n",
    "\n",
    "\n",
    "\n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Paraphrase_Generator/quora_raw_train.json') as f:\n",
    "    training_examples = json.load(f)\n",
    "    \n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Paraphrase_Generator/quora_raw_val.json') as f:\n",
    "    val_examples = json.load(f)\n",
    "\n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Paraphrase_Generator/quora_data_prepro.json') as f:\n",
    "    vocab = json.load(f)['ix_to_word'] # Vocab = {1: 'the', 2: 'sauce' .....}\n",
    "vocab_inv = {word:number for number,word in list(vocab.items())} # Inverted dictionary created   Vocab_inv = {'the':1, 'sauce':2, .....}\n",
    "\n",
    "X_train_prelim = [training_examples[i]['question1'] for i in range(len(training_examples))] # Training examples before preprocessing\n",
    "Y_train_prelim = [training_examples[i]['question'] for i in range(len(training_examples))]\n",
    "\n",
    "X_val_prelim = [val_examples[i]['question1'] for i in range(len(val_examples))]\n",
    "Y_val_prelim = [val_examples[i]['question'] for i in range(len(val_examples))]\n",
    "\n",
    "\n",
    "\n",
    "def load_glove_embeddings(filename=\"glove.6B.100d.txt\"):\n",
    "    try:\n",
    "        with open(\"/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Paraphrase_Generator/embeddings\", \"rb\") as f:\n",
    "            embeddings = pickle.load(f)\n",
    "        return embeddings\n",
    "    except:\n",
    "        lines = open(filename).readlines()\n",
    "        print(len(lines))\n",
    "        embeddings = {}\n",
    "        a = 0\n",
    "        for line in lines:\n",
    "            a +=1\n",
    "            print(a)\n",
    "            word = line.split()[0]\n",
    "            embedding = list(map(float, line.split()[1:]))\n",
    "            if word in vocab.values():\n",
    "                embeddings[int(vocab_inv[word])] = embedding # This is creating a dictionary with indexes corresponding to the position of the particular word in the vocabulary\n",
    "                # embeddings = {1:embedding of 'the', 2: embedding of 'sauce'....}\n",
    "        return embeddings\n",
    "\n",
    "def tokenize_embed(X,Y): # Doing this for training and validation_datasets\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for i in range(len(X)): # This is basically tokenising each sentence, and converting it into a list of word embeddings. X corresponds to a training example and Y corresponds to its paraphrase\n",
    "        print(i)\n",
    "        sentence_x = X[i]\n",
    "        sentence_y = Y[i]\n",
    "        tokensx = word_tokenize(sentence_x.lower())\n",
    "        tokensy = word_tokenize(sentence_y.lower())\n",
    "        vectorx = []\n",
    "        vectory = []\n",
    "        for word in tokensx:\n",
    "            try:\n",
    "                vectorx.append(int(vocab_inv[word]))\n",
    "            except:\n",
    "                vectorx.append(258) # If the word is not in the embeddings given, then add a ',' instead. (Need to discuss this)\n",
    "        for word in tokensy:\n",
    "            try:\n",
    "                vectory.append(int(vocab_inv[word]))\n",
    "            except:\n",
    "                vectory.append(258)\n",
    "        X_train.append(vectorx)\n",
    "        Y_train.append(vectory)\n",
    "    return X_train,Y_train\n",
    "\n",
    "def clip_pad_input_sequences(X,Y,min_sentence_length,max_sentence_length): # Doing this for training and validation datasets\n",
    "    X_training_clipped = []\n",
    "    Y_training_clipped = []\n",
    "    for i in range(len(X)):\n",
    "        print(i,X[i])\n",
    "        if len(X[i]) >= min_sentence_length and len(X[i]) <= max_sentence_length and len(Y[i]) >= min_sentence_length and len(Y[i]) <= max_sentence_length:\n",
    "            X_training_clipped.append(torch.Tensor(X[i]))\n",
    "            Y_training_clipped.append(torch.Tensor(Y[i]))\n",
    "    padded_X_training_clipped = pad_sequence(X_training_clipped,padding_value = vocab_size)\n",
    "    padded_Y_training_clipped = pad_sequence(X_training_clipped,padding_value = vocab_size)\n",
    "    return padded_X_training_clipped,padded_Y_training_clipped\n",
    "\n",
    "def find_closest_embedding(output,vocab,embedded_words): # Need this because we probably need to find the closest embedding to a given embedding and thereby find the word\n",
    "    return\n",
    "\n",
    "min_sentence_length = 7\n",
    "max_sentence_length = 16\n",
    "vocab_size = len(vocab)\n",
    "embedded_words = load_glove_embeddings()\n",
    "\n",
    "X_train,Y_train = clip_pad_input_sequences(*tokenize_embed(X_train_prelim,Y_train_prelim,),min_sentence_length,max_sentence_length)\n",
    "X_valid,Y_valid = clip_pad_input_sequences(*tokenize_embed(X_val_prelim,Y_val_prelim,),min_sentence_length,max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### THIS IS THE PREPROCESSING THEY DID IN THE PAPER ####\n",
    "##### RUN THIS IF U WANT AFTER CHANGING THE DIRECTORIES ######\n",
    "\n",
    "\n",
    "def process_data(data, data_len):\n",
    "    N = data.size()[0]\n",
    "    new_data = torch.zeros(N, data.size()[1] + 2, dtype=torch.long) + PAD_token\n",
    "    for i in range(N):\n",
    "        new_data[i, 1:data_len[i]+1] = data[i, :data_len[i]]\n",
    "        new_data[i, 0] = SOS_token\n",
    "        new_data[i, data_len[i]+1] = EOS_token\n",
    "        data_len[i] += 2\n",
    "    return new_data, data_len\n",
    "\n",
    "import h5py\n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Paraphrase_Generator/quora_data_prepro.json') as input_file:\n",
    "    data_dict = json.load(input_file)\n",
    "\n",
    "ix_to_word = {}\n",
    "\n",
    "for k in data_dict['ix_to_word']:\n",
    "    ix_to_word[int(k)] = data_dict['ix_to_word'][k]\n",
    "\n",
    "UNK_token = 0\n",
    "\n",
    "if 0 not in ix_to_word:\n",
    "    ix_to_word[0] = '<UNK>'\n",
    "\n",
    "else :\n",
    "    raise Exception\n",
    "\n",
    "EOS_token = len(ix_to_word)\n",
    "ix_to_word[EOS_token] = '<EOS>'\n",
    "PAD_token = len(ix_to_word)\n",
    "ix_to_word[PAD_token] = '<PAD>'\n",
    "SOS_token = len(ix_to_word)\n",
    "ix_to_word[SOS_token] = '<SOS>'\n",
    "vocab_size = len(ix_to_word)\n",
    "print('DataLoader loading h5 question file:', '/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/quora_data_prepro.h5')\n",
    "qa_data = h5py.File('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/quora_data_prepro.h5', 'r')\n",
    "\n",
    "ques_id_train = torch.from_numpy(qa_data['ques_cap_id_train'][...].astype(int))\n",
    "\n",
    "ques_train, ques_len_train = process_data(torch.from_numpy(qa_data['ques_train'][...].astype(int)), torch.from_numpy(qa_data['ques_length_train'][...].astype(int)))\n",
    "\n",
    "label_train, label_len_train = process_data(torch.from_numpy(qa_data['ques1_train'][...].astype(int)), torch.from_numpy(qa_data['ques1_length_train'][...].astype(int)))\n",
    "\n",
    "train_id = 0\n",
    "seq_length = ques_train.size()[1]\n",
    "\n",
    "print('Training dataset length : ', ques_train.size()[0])\n",
    "\n",
    "\n",
    "ques_test, ques_len_test = process_data(torch.from_numpy(qa_data['ques_test'][...].astype(int)), torch.from_numpy(qa_data['ques_length_test'][...].astype(int)))\n",
    "\n",
    "label_test, label_len_test = process_data(torch.from_numpy(qa_data['ques1_test'][...].astype(int)), torch.from_numpy(qa_data['ques1_length_test'][...].astype(int)))\n",
    "\n",
    "ques_id_test = torch.from_numpy(qa_data['ques_cap_id_test'][...].astype(int))\n",
    "\n",
    "test_id = 0\n",
    "\n",
    "print('Test dataset length : ', ques_test.size()[0])\n",
    "qa_data.close()\n",
    "\n",
    "ques = torch.cat([ques_train, ques_test])\n",
    "len = torch.cat([ques_len_train, ques_len_test])\n",
    "label = torch.cat([label_train, label_test])\n",
    "label_len = torch.cat([label_len_train, label_len_test])\n",
    "id = torch.cat([ques_id_train, ques_id_test])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27698, 21519, 16077,  7703,  8223, 23290, 25099, 22596, 17182, 27696,\n",
      "        27697, 27697, 27697, 27697, 27697, 27697, 27697, 27697, 27697, 27697,\n",
      "        27697, 27697, 27697, 27697, 27697, 27697, 27697, 27697])\n"
     ]
    }
   ],
   "source": [
    "#### CAN PICKLE.DUMP THIS SHIZZZ #####\n",
    "X_train,Y_train = ques_train,label_train\n",
    "a = (X_train,Y_train)\n",
    "X_valid,Y_valid = ques_test,label_test\n",
    "b = (X_valid,Y_valid)\n",
    "import pickle\n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Training_Examples','wb') as f:\n",
    "    pickle.dump(a,f)\n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Validation_Examples','wb') as f:\n",
    "    pickle.dump(b,f)\n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Vocab_Extra','wb') as f:\n",
    "    pickle.dump(ix_to_word,f)\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)  \n",
    "print(X_train[0])\n",
    "def one_hot(t, c):\n",
    "    return torch.zeros(*t.size(), c, device=t.device).scatter_(-1, t.unsqueeze(-1), 1)\n",
    "a = X_train[0]\n",
    "b = one_hot(a, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 28 27699\n",
      "0\n",
      "torch.Size([28, 27699])\n",
      "1\n",
      "torch.Size([28, 27699])\n",
      "2\n",
      "torch.Size([28, 27699])\n",
      "3\n",
      "torch.Size([28, 27699])\n",
      "4\n",
      "torch.Size([28, 27699])\n",
      "5\n",
      "torch.Size([28, 27699])\n",
      "6\n",
      "torch.Size([28, 27699])\n",
      "7\n",
      "torch.Size([28, 27699])\n",
      "8\n",
      "torch.Size([28, 27699])\n",
      "9\n",
      "torch.Size([28, 27699])\n",
      "10\n",
      "torch.Size([28, 27699])\n",
      "11\n",
      "torch.Size([28, 27699])\n",
      "12\n",
      "torch.Size([28, 27699])\n",
      "13\n",
      "torch.Size([28, 27699])\n",
      "14\n",
      "torch.Size([28, 27699])\n",
      "15\n",
      "torch.Size([28, 27699])\n",
      "16\n",
      "torch.Size([28, 27699])\n",
      "17\n",
      "torch.Size([28, 27699])\n",
      "18\n",
      "torch.Size([28, 27699])\n",
      "19\n",
      "torch.Size([28, 27699])\n",
      "20\n",
      "torch.Size([28, 27699])\n",
      "21\n",
      "torch.Size([28, 27699])\n",
      "22\n",
      "torch.Size([28, 27699])\n",
      "23\n",
      "torch.Size([28, 27699])\n",
      "24\n",
      "torch.Size([28, 27699])\n",
      "25\n",
      "torch.Size([28, 27699])\n",
      "26\n",
      "torch.Size([28, 27699])\n",
      "27\n",
      "torch.Size([28, 27699])\n",
      "28\n",
      "torch.Size([28, 27699])\n",
      "29\n",
      "torch.Size([28, 27699])\n",
      "30\n",
      "torch.Size([28, 27699])\n",
      "31\n",
      "torch.Size([28, 27699])\n",
      "32\n",
      "torch.Size([28, 27699])\n",
      "33\n",
      "torch.Size([28, 27699])\n",
      "34\n",
      "torch.Size([28, 27699])\n",
      "35\n",
      "torch.Size([28, 27699])\n",
      "36\n",
      "torch.Size([28, 27699])\n",
      "37\n",
      "torch.Size([28, 27699])\n",
      "38\n",
      "torch.Size([28, 27699])\n",
      "39\n",
      "torch.Size([28, 27699])\n",
      "40\n",
      "torch.Size([28, 27699])\n",
      "41\n",
      "torch.Size([28, 27699])\n",
      "42\n",
      "torch.Size([28, 27699])\n",
      "43\n",
      "torch.Size([28, 27699])\n",
      "44\n",
      "torch.Size([28, 27699])\n",
      "45\n",
      "torch.Size([28, 27699])\n",
      "46\n",
      "torch.Size([28, 27699])\n",
      "47\n",
      "torch.Size([28, 27699])\n",
      "48\n",
      "torch.Size([28, 27699])\n",
      "49\n",
      "torch.Size([28, 27699])\n",
      "50\n",
      "torch.Size([28, 27699])\n",
      "51\n",
      "torch.Size([28, 27699])\n",
      "52\n",
      "torch.Size([28, 27699])\n",
      "53\n",
      "torch.Size([28, 27699])\n",
      "54\n",
      "torch.Size([28, 27699])\n",
      "55\n",
      "torch.Size([28, 27699])\n",
      "56\n",
      "torch.Size([28, 27699])\n",
      "57\n",
      "torch.Size([28, 27699])\n",
      "58\n",
      "torch.Size([28, 27699])\n",
      "59\n",
      "torch.Size([28, 27699])\n",
      "60\n",
      "torch.Size([28, 27699])\n",
      "61\n",
      "torch.Size([28, 27699])\n",
      "62\n",
      "torch.Size([28, 27699])\n",
      "63\n",
      "torch.Size([28, 27699])\n",
      "64\n",
      "torch.Size([28, 27699])\n",
      "65\n",
      "torch.Size([28, 27699])\n",
      "66\n",
      "torch.Size([28, 27699])\n",
      "67\n",
      "torch.Size([28, 27699])\n",
      "68\n",
      "torch.Size([28, 27699])\n",
      "69\n",
      "torch.Size([28, 27699])\n",
      "70\n",
      "torch.Size([28, 27699])\n",
      "71\n",
      "torch.Size([28, 27699])\n",
      "72\n",
      "torch.Size([28, 27699])\n",
      "73\n",
      "torch.Size([28, 27699])\n",
      "74\n",
      "torch.Size([28, 27699])\n",
      "75\n",
      "torch.Size([28, 27699])\n",
      "76\n",
      "torch.Size([28, 27699])\n",
      "77\n",
      "torch.Size([28, 27699])\n",
      "78\n",
      "torch.Size([28, 27699])\n",
      "79\n",
      "torch.Size([28, 27699])\n",
      "80\n",
      "torch.Size([28, 27699])\n",
      "81\n",
      "torch.Size([28, 27699])\n",
      "82\n",
      "torch.Size([28, 27699])\n",
      "83\n",
      "torch.Size([28, 27699])\n",
      "84\n",
      "torch.Size([28, 27699])\n",
      "85\n",
      "torch.Size([28, 27699])\n",
      "86\n",
      "torch.Size([28, 27699])\n",
      "87\n",
      "torch.Size([28, 27699])\n",
      "88\n",
      "torch.Size([28, 27699])\n",
      "89\n",
      "torch.Size([28, 27699])\n",
      "90\n",
      "torch.Size([28, 27699])\n",
      "91\n",
      "torch.Size([28, 27699])\n",
      "92\n",
      "torch.Size([28, 27699])\n",
      "93\n",
      "torch.Size([28, 27699])\n",
      "94\n",
      "torch.Size([28, 27699])\n",
      "95\n",
      "torch.Size([28, 27699])\n",
      "96\n",
      "torch.Size([28, 27699])\n",
      "97\n",
      "torch.Size([28, 27699])\n",
      "98\n",
      "torch.Size([28, 27699])\n",
      "99\n",
      "torch.Size([28, 27699])\n",
      "100\n",
      "torch.Size([28, 27699])\n",
      "101\n",
      "torch.Size([28, 27699])\n",
      "102\n",
      "torch.Size([28, 27699])\n",
      "103\n",
      "torch.Size([28, 27699])\n",
      "104\n",
      "torch.Size([28, 27699])\n",
      "105\n",
      "torch.Size([28, 27699])\n",
      "106\n",
      "torch.Size([28, 27699])\n",
      "107\n",
      "torch.Size([28, 27699])\n",
      "108\n",
      "torch.Size([28, 27699])\n",
      "109\n",
      "torch.Size([28, 27699])\n",
      "110\n",
      "torch.Size([28, 27699])\n",
      "111\n",
      "torch.Size([28, 27699])\n",
      "112\n",
      "torch.Size([28, 27699])\n",
      "113\n",
      "torch.Size([28, 27699])\n",
      "114\n",
      "torch.Size([28, 27699])\n",
      "115\n",
      "torch.Size([28, 27699])\n",
      "116\n",
      "torch.Size([28, 27699])\n",
      "117\n",
      "torch.Size([28, 27699])\n",
      "118\n",
      "torch.Size([28, 27699])\n",
      "119\n",
      "torch.Size([28, 27699])\n",
      "120\n",
      "torch.Size([28, 27699])\n",
      "121\n",
      "torch.Size([28, 27699])\n",
      "122\n",
      "torch.Size([28, 27699])\n",
      "123\n",
      "torch.Size([28, 27699])\n",
      "124\n",
      "torch.Size([28, 27699])\n",
      "125\n",
      "torch.Size([28, 27699])\n",
      "126\n",
      "torch.Size([28, 27699])\n",
      "127\n",
      "torch.Size([28, 27699])\n"
     ]
    }
   ],
   "source": [
    "##### RUN THIS TO SEE THE OUTPUT ####\n",
    "##### THEY HAVE ADDED EXTRA TOKENS ####\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Training_Examples','rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Validation_Examples','rb') as f:\n",
    "    val = pickle.load(f)\n",
    "with open('/Users/varunbabbar/Desktop/Good_Quotes/Paraphrase_Generator/Vocab_Extra','rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "X_train,Y_train = train[0],train[1]\n",
    "print(len(X_train),X_train[0].shape[0],len(vocab))\n",
    "X_train_final = torch.zeros(len(X_train),X_train[0].shape[0],len(vocab)) \n",
    "Y_train_final = torch.zeros(len(Y_train),Y_train[0].shape[0],len(vocab))\n",
    "for i in range(128):\n",
    "    print(i)\n",
    "    a = one_hot(X_train[i], vocab_size) \n",
    "    print(a.shape)\n",
    "    X_train_final[i,:,:] = one_hot(X_train[i], len(vocab))\n",
    "    Y_train_final[i,:,:] = one_hot(Y_train[i], len(vocab))\n",
    "# for i in range\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}