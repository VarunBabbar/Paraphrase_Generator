{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import json \n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def loadData():\n",
    "\n",
    "    with open('quora_raw_train.json') as f:\n",
    "        training_examples = json.load(f)\n",
    "        \n",
    "    with open('quora_raw_val.json') as f:\n",
    "        val_examples = json.load(f)\n",
    "\n",
    "    with open('quora_data_prepro.json') as f:\n",
    "        vocab = json.load(f)['ix_to_word'] # Vocab = {1: 'the', 2: 'sauce' .....}\n",
    "        vocab_inv = {word:number for number,word in list(vocab.items())} # Inverted dictionary created          Vocab_inv = {'the':1, 'sauce':2, .....}\n",
    "\n",
    "    X_train_prelim = [training_examples[i]['question1'] for i in range(len(training_examples))] # Training examples before preprocessing\n",
    "    Y_train_prelim = [training_examples[i]['question'] for i in range(len(training_examples))]\n",
    "\n",
    "    X_val_prelim = [val_examples[i]['question1'] for i in range(len(val_examples))]\n",
    "    Y_val_prelim = [val_examples[i]['question'] for i in range(len(val_examples))]\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "\n",
    "    filename = \"glove.6B.100d.txt\"\n",
    "\n",
    "    lines = open(filename).readlines()\n",
    "    print(len(lines))\n",
    "    embeddings = {}\n",
    "    a = 0\n",
    "    for line in lines:\n",
    "        a +=1\n",
    "        print(a)\n",
    "        word = line.split()[0]\n",
    "        embedding = list(map(float, line.split()[1:]))\n",
    "        if word in vocab.values():\n",
    "            embeddings[int(vocab_inv[word])] = embedding # This is creating a dictionary with indexes corresponding to the position of the particular word in the vocabulary\n",
    "            # embeddings = {1:embedding of 'the', 2: embedding of 'sauce'....}\n",
    "        \n",
    "    embedded_words = embeddings\n",
    "    \n",
    "# This is basically tokenising each sentence, and converting it into a list of word embeddings. X corresponds to a training example and Y corresponds to its paraphrase\n",
    "    for i in range(len(X_train_prelim)): \n",
    "        print(i)\n",
    "        sentence_x = X_train_prelim[i]\n",
    "        sentence_y = Y_train_prelim[i]\n",
    "        tokensx = word_tokenize(sentence_x)\n",
    "        tokensy = word_tokenize(sentence_y)\n",
    "        vectorx = []\n",
    "        vectory = []\n",
    "        for word in tokensx:\n",
    "            try:\n",
    "                vectorx.append(embedded_words[int(vocab_inv[word])])\n",
    "            except:\n",
    "#If the word is not in the embeddings given, then add a random vector (might have to do something about this :) ) (Potentially could write a spell checker to find misspellings)\n",
    "                vectorx.append(-2*torch.rand(100,1)) #\n",
    "        for word in tokensy:\n",
    "            try:\n",
    "                vectorx.append(embedded_words[int(vocab_inv[word])])\n",
    "            except:\n",
    "                vectory.append(-2*torch.rand(100,1))\n",
    "\n",
    "        X_train.append(vectorx)\n",
    "        Y_train.append(vectory)\n",
    "\n",
    "    return embedded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'quora_raw_val.json'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-562a4063ff4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0membedded_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-700cdfe299d0>\u001b[0m in \u001b[0;36mloadData\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtraining_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'quora_raw_val.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mval_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'quora_raw_val.json'"
     ]
    }
   ],
   "source": [
    "from Decoder import Decoder\n",
    "from Encoder import Encoder\n",
    "\n",
    "input_size = 100\n",
    "hidden_size_enc = 128\n",
    "hidden_size_dec = 20\n",
    "num_layers = 1\n",
    "vocab_size = 5 #len(vocab)\n",
    "dropout = 0.2\n",
    "\n",
    "embedded_words = loadData()\n",
    "\n",
    "encoder = Encoder(input_size,hidden_size_enc,dropout,vocab_size,num_layers, embedded_words)\n",
    "decoder = Decoder(encoder, input_dims, hidden_size_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit7d7fb46cf8184372a0eba756136f58f4",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}